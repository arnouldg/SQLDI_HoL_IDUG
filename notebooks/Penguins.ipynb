{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with Penguins\n",
    "\n",
    "* The dataset contains several measurements (features) of 344 penguins across 3 antartic islands\n",
    "* It contains a species designation (the label).\n",
    "* It is frequently used for learning how to develop machine learning classification models.\n",
    "* This Jupyter notebook contains data science work to analyze the data and train classification models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TestImage](img/penguins3.JPG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Science Project Samples\n",
    "There are many sample data science projects to download from the internet.\n",
    "These examples are based upon a couple of projects published in Kaggle at the links below.\n",
    "\n",
    "* https://www.kaggle.com/code/parulpandey/penguin-dataset-the-new-iris/notebook\n",
    "* https://www.kaggle.com/code/mohamedharris/how-to-classify-penguins-a-beginner-s-guide\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Connect to Db2 z/OS and review the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ipython-sql==0.4.1 ibm_db sqlalchemy==1.4.47 ibm_db_sa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and import them...\n",
    "\n",
    "import sys,os,os.path\n",
    "os.environ['IBM_DB_HOME']='C:\\Program Files\\IBM\\SQLLIB'\n",
    "\n",
    "import ibm_db \n",
    "import ibm_db_sa \n",
    "import sqlalchemy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets take a look at the Penguins Dataset \n",
    "%load_ext sql\n",
    "%sql ibm_db_sa://IBMUSER:SYS1@wg31.washington.ibm.com:5045/DALLASD\n",
    "%sql select * from EXPLORE.PENGUINS fetch first 10 rows only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick count of the data records by island, species and sex\n",
    "\n",
    "%sql select island, species, sex, count(*) as count from explore.penguins group by island, species, sex order by island, species, sex"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Install and Import the standard data science tools you wish to use\n",
    "\n",
    "* Libraries exist for many purposes (data access, mathematical tools, visualisation tools etc...)\n",
    "* The libraries that you choose to use must be installed and imported into this python kernel (cw01 - top right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas dcor numpy scikit-learn matplotlib seaborn pycountry plotly cufflinks folium nbformat chart_studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries you want to use\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import dcor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Visualisation libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import pycountry\n",
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode, iplot \n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "from plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\n",
    "!pip install chart_studio\n",
    "import chart_studio.plotly as py\n",
    "import cufflinks\n",
    "cufflinks.go_offline()\n",
    "cufflinks.set_config_file(world_readable=True, theme='pearl')\n",
    "#py.init_notebook_mode(connected=True)\n",
    "\n",
    "#Geographical Plotting\n",
    "import folium\n",
    "from folium import Choropleth, Circle, Marker\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap, MarkerCluster\n",
    "\n",
    "#Racing Bar Chart\n",
    "!pip install bar_chart_race\n",
    "import bar_chart_race as bcr\n",
    "from IPython.display import HTML\n",
    "\n",
    "import nbformat \n",
    "\n",
    "# Increase the default plot size and set the color scheme\n",
    "plt.rcParams['figure.figsize'] = 8, 5\n",
    "plt.style.use(\"fivethirtyeight\")# for pretty graphs\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Disable warnings \n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Create and Populate a dataframe\n",
    "\n",
    "* a DataFrame is a 2-dimensional labeled data structure with columns of potentially different types.\n",
    "* It's conceptually the same as a Db2 table, but exists in the Python kernel for the data scientist to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a variable that contains a string of your credentials\n",
    "credentials = \"ibm_db_sa://IBMUSER:SYS1@wg31.washington.ibm.com:5045/DALLASD\"\n",
    "\n",
    "# read in your SQL query results using pandas\n",
    "dataframe = pd.read_sql(\"\"\"\n",
    "            select species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex, year \n",
    "            from explore.penguins\n",
    "            ORDER BY id\n",
    "            \"\"\", con = credentials)\n",
    "\n",
    "# and save the original\n",
    "original = dataframe.copy() \n",
    "\n",
    "# return your first ten rows\n",
    "dataframe.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the fields in the dataframe\n",
    "\n",
    "dataframe.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate some numercial average and distribution data\n",
    "\n",
    "dataframe.describe(include='all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Start \"Data Wrangling\" with pandas\n",
    "\n",
    "Data Wrangling consists of 5 main activity areas:\n",
    "\n",
    "1. Data exploration — feature values, ranges, correlations, relationships...\n",
    "2. Dealing with missing values — various strategies\n",
    "3. Reshaping data — pivot tables, joins, grouping and aggregating\n",
    "4. Filtering data - selection, projection etc...\n",
    "5. Other — Making descriptive columns, element-wise conditional operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of each species in the labelled dataset\n",
    "\n",
    "dataframe['species'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or represent the counts graphically\n",
    "\n",
    "dataframe['species'].value_counts().iplot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a filtered dataframe for exploring ranges of data values for different species of penguin\n",
    "\n",
    "df1 = dataframe[['species', 'bill_length_mm', 'bill_depth_mm','flipper_length_mm']]\n",
    "sns.boxplot(data=df1, width=0.5,fliersize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, focus on bill length by species and sex.\n",
    "\n",
    "def box(f):\n",
    "    sns.boxplot(y = f, x = 'species', hue = 'sex',data = dataframe)\n",
    "    plt.title(f)\n",
    "    plt.show() \n",
    "\n",
    "box('bill_length_mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each of the features are helpful in differentiating between species. Combining multiple features can expose stronger data patterns.\n",
    "\n",
    "sns.pairplot(df1, hue=\"species\", height=3,diag_kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can zoom in on particular pairplots\n",
    "\n",
    "sns.FacetGrid(df1, hue=\"species\", height=8) \\\n",
    "   .map(plt.scatter, \"bill_length_mm\", \"flipper_length_mm\") \\\n",
    "   .add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another visualisation tool is the violin plot\n",
    "\n",
    "ax = sns.violinplot(x=\"species\", y=\"flipper_length_mm\", data=df1,height=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we can plot distribution curves\n",
    "\n",
    "sns.FacetGrid(df1, hue=\"species\", height=6,) \\\n",
    "   .map(sns.kdeplot, \"flipper_length_mm\",fill=True) \\\n",
    "   .add_legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : Cleaning the Data\n",
    "\n",
    "The previous section showed a very small subset of data wrangling techniques to allow the data scientist to understand the dataset, see patterns in the data, and start to form ideas about which model types may be best suited to the task of creating a classification model.\n",
    "\n",
    "Before fitting and training a model, a data pipeline should be built to cleanse and transform the data into a suitable form. Data cleansing work may include\n",
    "* identifying null or empty values, and deciding on a technique to handle them\n",
    "* eliminating data features that don't have a big impact on the classification of records\n",
    "* converting string datatypes into numerical types\n",
    "* Normalising the data values into ranges with good distribution of values\n",
    "etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries for 5 model types\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eyeball the first 10 rows. We can see nulls and missing values.\n",
    "\n",
    "dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the counts of Null values\n",
    "\n",
    "dataframe.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe and choose statistical means for missing values\n",
    "# in the example below we just choose the statistical mean value for the 4 measurement data features, and the modal value for sex data feature.\n",
    "\n",
    "new_df = original.copy()\n",
    "\n",
    "new_df['bill_length_mm'].fillna(np.mean(original['bill_length_mm']), inplace = True)\n",
    "new_df['bill_depth_mm'].fillna(np.mean(original['bill_depth_mm']), inplace = True)\n",
    "new_df['flipper_length_mm'].fillna(np.mean(original['flipper_length_mm']), inplace = True)\n",
    "new_df['body_mass_g'].fillna(np.mean(original['body_mass_g']), inplace = True)\n",
    "new_df['sex'].fillna(original['sex'].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that we have assigned values to all the missing data features\n",
    "\n",
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and check that the null values are missing\n",
    "\n",
    "new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skewness is a statistical measure of asymmetry from a normal distribution. Skewness between -0.5 and +0.5 is considered symmetrical.\n",
    "# this dataset does not suffer from skewing\n",
    "\n",
    "print('Skewness of numeric variables')\n",
    "print('-' * 35)\n",
    "\n",
    "for i in new_df.select_dtypes(['int64', 'float64']).columns.tolist():\n",
    "    print(i, ' : ',new_df[i].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now take a look at the range of actual values for the data features.new_df\n",
    "\n",
    "new_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the goal of data normalization is to ensure that data is similar across all records, to make the model more efficient\n",
    "# The function below is used to normalse the numerical data features between the values of -1 and +1.\n",
    "# See how it compresses the data values. (aka \"squishification\")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "new_df['bill_length_mm'] = mms.fit_transform(new_df['bill_length_mm'].values.reshape(-1, 1))\n",
    "new_df['bill_depth_mm'] = mms.fit_transform(new_df['bill_depth_mm'].values.reshape(-1, 1))\n",
    "new_df['flipper_length_mm'] = mms.fit_transform(new_df['flipper_length_mm'].values.reshape(-1, 1))\n",
    "new_df['body_mass_g'] = mms.fit_transform(new_df['body_mass_g'].values.reshape(-1, 1))\n",
    "new_df['sex'].fillna(original['sex'].mode()[0], inplace = True)\n",
    "\n",
    "new_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets create dummy values for sex and island fields\n",
    "\n",
    "new_df_dummy = pd.get_dummies(new_df, columns = ['sex', 'island'], drop_first = True) \n",
    "\n",
    "new_df_dummy.head(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets assign numerical values for the 3 species. First we check the unique text values that exist.\n",
    "\n",
    "new_df_dummy['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we replace those values with numerical surrogate values.\n",
    "\n",
    "new_df_dummy['species'].replace(['Adelie                        ',\n",
    "                                  'Chinstrap                     ',\n",
    "                                    'Gentoo                        '],\n",
    "                                     [0, 1, 2], inplace = True)\n",
    "\n",
    "new_df_dummy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sns.heatmap(new_df_dummy.corr(), annot = True, cmap = 'Blues')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 sets of data, with and without the label\n",
    "\n",
    "X = new_df_dummy.drop(columns = ['species'])\n",
    "Y = new_df_dummy['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data 75% training and 25% test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with a Logical Regression Model\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train, Y_train)\n",
    "\n",
    "pred = LR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Accuracy and F1 Score of the trained model\n",
    "\n",
    "print('Accuracy : ', accuracy_score(Y_test, pred))\n",
    "print('F1 Score : ', f1_score(Y_test, pred, average = 'weighted')) \n",
    "print('Precision : ', precision_score(Y_test, pred , average = 'weighted'))\n",
    "print('Recall : ', recall_score(Y_test, pred, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try some other candidate models \n",
    "\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('kNN', KNeighborsClassifier()))\n",
    "models.append(('SVC', SVC()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cw01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
